{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb6346db-c6d0-4125-857e-f76a613c2a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import argparse\n",
    "#from models import *\n",
    "#from utils1 import progress_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82ee928d-8125-4e73-aead-bf8fd41a72c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cc57f41-2ab2-4893-8897-dadfa80d2682",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30866653-59de-42f9-beac-393dac691d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pzalavad/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31bc2449-48d1-4bb5-a1a4-94fb739e7a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import time\n",
    "import sys\n",
    "\n",
    "\n",
    "def get_mean_and_std(dataset):\n",
    "    '''Compute the mean and std value of dataset.'''\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n",
    "    mean = torch.zeros(3)\n",
    "    std = torch.zeros(3)\n",
    "    print('==> Computing mean and std..')\n",
    "    for inputs, targets in dataloader:\n",
    "        for i in range(3):\n",
    "            mean[i] += inputs[:,i,:,:].mean()\n",
    "            std[i] += inputs[:,i,:,:].std()\n",
    "    mean.div_(len(dataset))\n",
    "    std.div_(len(dataset))\n",
    "    return mean, std\n",
    "\n",
    "def init_params(net):\n",
    "    '''Init layer parameters.'''\n",
    "    for m in net.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "            if m.bias is not None:\n",
    "                init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            init.constant_(m.weight, 1)\n",
    "            init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            init.normal_(m.weight, std=1e-3)\n",
    "            if m.bias is not None:\n",
    "                init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "TOTAL_BAR_LENGTH = 65.\n",
    "last_time = time.time()\n",
    "begin_time = last_time\n",
    "def progress_bar(current, total, msg=None):\n",
    "    global last_time, begin_time\n",
    "    if current == 0:\n",
    "        begin_time = time.time()  # Reset for new bar.\n",
    "\n",
    "    cur_len = int(TOTAL_BAR_LENGTH*current/total)\n",
    "    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n",
    "\n",
    "    sys.stdout.write(' [')\n",
    "    for i in range(cur_len):\n",
    "        sys.stdout.write('=')\n",
    "    sys.stdout.write('>')\n",
    "    for i in range(rest_len):\n",
    "        sys.stdout.write('.')\n",
    "    sys.stdout.write(']')\n",
    "\n",
    "    cur_time = time.time()\n",
    "    step_time = cur_time - last_time\n",
    "    last_time = cur_time\n",
    "    tot_time = cur_time - begin_time\n",
    "\n",
    "    L = []\n",
    "    L.append('  Step: %s' % format_time(step_time))\n",
    "    L.append(' | Tot: %s' % format_time(tot_time))\n",
    "    if msg:\n",
    "        L.append(' | ' + msg)\n",
    "\n",
    "    msg = ''.join(L)\n",
    "    sys.stdout.write(msg)\n",
    "    for i in range(int(TOTAL_BAR_LENGTH) - len(msg)):\n",
    "        sys.stdout.write(' ')\n",
    "\n",
    "    # Go back to the center of the bar.\n",
    "    for i in range(int(TOTAL_BAR_LENGTH/2) + 7):\n",
    "        sys.stdout.write('\\b')\n",
    "    sys.stdout.write(' %d/%d ' % (current+1, total))\n",
    "\n",
    "    if current < total-1:\n",
    "        sys.stdout.write('\\r')\n",
    "    else:\n",
    "        sys.stdout.write('\\n')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def format_time(seconds):\n",
    "    days = int(seconds / 3600/24)\n",
    "    seconds = seconds - days*3600*24\n",
    "    hours = int(seconds / 3600)\n",
    "    seconds = seconds - hours*3600\n",
    "    minutes = int(seconds / 60)\n",
    "    seconds = seconds - minutes*60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06c205c-8e6f-4110-8f26-a9af1e38570a",
   "metadata": {},
   "source": [
    "Model Parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca79c15b-5f8f-4ae3-b464-f04d43d0d117",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride = 1, downsample = None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "                        nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = stride, padding = 1),\n",
    "                        nn.BatchNorm2d(out_channels),\n",
    "                        nn.ReLU())\n",
    "        self.conv2 = nn.Sequential(\n",
    "                        nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1),\n",
    "                        nn.BatchNorm2d(out_channels))\n",
    "        self.downsample = downsample\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inplanes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=1)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(4, stride=1)\n",
    "        self.linear = nn.Linear(512, num_classes)\n",
    "        \n",
    "#         self.conv1.to('cuda:0')\n",
    "#         self.bn1.to('cuda:0')\n",
    "#         self.maxpool.to('cuda:0')\n",
    "#         self.layer1.to('cuda:0')\n",
    "#         self.layer2.to('cuda:0')\n",
    "\n",
    "#         # Move the remaining layers to device 1\n",
    "#         self.layer3.to('cuda:1')\n",
    "#         self.layer4.to('cuda:1')\n",
    "#         self.avgpool.to('cuda:1')\n",
    "#         self.linear.to('cuda:1')\n",
    "        \n",
    "        # model para\n",
    "        self.seq1 = nn.Sequential(\n",
    "            self.conv1,\n",
    "            self.bn1,\n",
    "            self.relu,\n",
    "            self.maxpool,\n",
    "            self.layer1,\n",
    "            self.layer2\n",
    "        ).to('cuda:0')\n",
    "\n",
    "        self.seq2 = nn.Sequential(\n",
    "            self.layer3,\n",
    "            self.layer4,\n",
    "            self.avgpool,\n",
    "        ).to('cuda:1')\n",
    "        \n",
    "        self.linear.to('cuda:1')\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes:\n",
    "            \n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(planes),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x.to('cuda:0')\n",
    "        # x = self.conv1(x)\n",
    "        # x = self.bn1(x)\n",
    "        # x = self.relu(x)\n",
    "        # x = self.maxpool(x)\n",
    "        # x = self.layer1(x)\n",
    "        # x = self.layer2(x)\n",
    "        # x = x.to('cuda:1')\n",
    "        # x = self.layer3(x)\n",
    "        # x = self.layer4(x)\n",
    "        # x = self.avgpool(x)\n",
    "        # x = x.view(x.size(0), -1)\n",
    "        # x = self.linear(x)\n",
    "        # return x\n",
    "        x = self.seq2(self.seq1(x.to('cuda:0')).to('cuda:1'))\n",
    "        return self.linear(x.view(x.size(0), -1))\n",
    "    \n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(ResidualBlock, [3, 4, 6, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dd1232a-df87-4b97-ae4e-2d68789893b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building model..\n",
      "==> Model Build Completed.\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "print('==> Building model..')\n",
    "net = ResNet50()\n",
    "#net = net.to('cuda:0')  \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr,momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "print('==> Model Build Completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8d197cc-b403-4a73-8d20-8333e715e201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      " [================================================================>]  Step: None | Tot: None | Loss: 1.584 | Acc: 42.100% (21050/50000 391/391 .............................................................]  Step: None | Tot: None | Loss: 2.407 | Acc: 11.545% (133/1152) 9/391 ===============================================>.................]  Step: None | Tot: None | Loss: 1.677 | Acc: 38.676% (14208/36736 287/391 =========================================================>.......]  Step: None | Tot: None | Loss: 1.627 | Acc: 40.523% (17895/44160 345/391 \n",
      " [================================================================>]  Step: None | Tot: None | Loss: 1.463 | Acc: 51.230% (5123/10000 100/100 ==========>....................................................]  Step: None | Tot: None | Loss: 1.449 | Acc: 51.500% (1030/2000 20/100 =========================>.......................................]  Step: None | Tot: None | Loss: 1.461 | Acc: 51.050% (2042/4000 40/100 ===============================>.................................]  Step: None | Tot: None | Loss: 1.459 | Acc: 51.180% (2559/5000 50/100 ==================================>..............................]  Step: None | Tot: None | Loss: 1.458 | Acc: 51.037% (2756/5400 54/100 ======================================>..........................]  Step: None | Tot: None | Loss: 1.458 | Acc: 51.117% (3067/6000 60/100 =============================================>...................]  Step: None | Tot: None | Loss: 1.463 | Acc: 50.986% (3620/7100 71/100 ==================================================>..............]  Step: None | Tot: None | Loss: 1.457 | Acc: 51.256% (3998/7800 78/100 ==========================================================>......]  Step: None | Tot: None | Loss: 1.464 | Acc: 51.165% (4656/9100 91/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 1\n",
      " [================================================================>]  Step: None | Tot: None | Loss: 1.121 | Acc: 60.494% (30247/50000 391/391 \n",
      " [================================================================>]  Step: None | Tot: None | Loss: 1.072 | Acc: 63.700% (6370/10000 100/100 ==================>............................................]  Step: None | Tot: None | Loss: 1.074 | Acc: 63.091% (2082/3300 33/100 ======================>..........................................]  Step: None | Tot: None | Loss: 1.085 | Acc: 62.833% (2262/3600 36/100 =======================>.........................................]  Step: None | Tot: None | Loss: 1.089 | Acc: 62.703% (2320/3700 37/100 =================================>...............................]  Step: None | Tot: None | Loss: 1.074 | Acc: 63.547% (3368/5300 53/100 ======================================>..........................]  Step: None | Tot: None | Loss: 1.078 | Acc: 63.583% (3815/6000 60/100 =========================================>.......................]  Step: None | Tot: None | Loss: 1.078 | Acc: 63.492% (4127/6500 65/100 ==================================================>..............]  Step: None | Tot: None | Loss: 1.071 | Acc: 63.782% (4975/7800 78/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 2\n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.890 | Acc: 68.976% (34488/50000 391/391 ===============================================>.................]  Step: None | Tot: None | Loss: 0.913 | Acc: 68.116% (25023/36736 287/391 \n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.953 | Acc: 68.400% (6840/10000 100/100 ==========>....................................................]  Step: None | Tot: None | Loss: 0.965 | Acc: 67.900% (1358/2000 20/100 ====================>............................................]  Step: None | Tot: None | Loss: 0.958 | Acc: 67.303% (2221/3300 33/100 =======================>.........................................]  Step: None | Tot: None | Loss: 0.964 | Acc: 67.270% (2489/3700 37/100 ===============================>.................................]  Step: None | Tot: None | Loss: 0.949 | Acc: 68.140% (3407/5000 50/100 ==================================>..............................]  Step: None | Tot: None | Loss: 0.947 | Acc: 68.259% (3686/5400 54/100 ==========================================>......................]  Step: None | Tot: None | Loss: 0.947 | Acc: 68.179% (4568/6700 67/100 ========================================================>........]  Step: None | Tot: None | Loss: 0.949 | Acc: 68.511% (6029/8800 88/100 =============================================================>...]  Step: None | Tot: None | Loss: 0.951 | Acc: 68.495% (6507/9500 95/100 =============================================================>...]  Step: None | Tot: None | Loss: 0.950 | Acc: 68.510% (6577/9600 96/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 3\n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.724 | Acc: 75.008% (37504/50000 391/391 \n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.716 | Acc: 75.580% (7558/10000 100/100 =========>.....................................................]  Step: None | Tot: None | Loss: 0.696 | Acc: 75.889% (1366/1800 18/100 =============>...................................................]  Step: None | Tot: None | Loss: 0.715 | Acc: 75.143% (1578/2100 21/100 ====================>............................................]  Step: None | Tot: None | Loss: 0.725 | Acc: 75.062% (2402/3200 32/100 ===========================>.....................................]  Step: None | Tot: None | Loss: 0.721 | Acc: 75.432% (3319/4400 44/100 ========================================>........................]  Step: None | Tot: None | Loss: 0.723 | Acc: 75.286% (4743/6300 63/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 4\n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.636 | Acc: 78.052% (39026/50000 391/391 =============================>...................................]  Step: None | Tot: None | Loss: 0.648 | Acc: 77.483% (17852/23040 180/391 \n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.701 | Acc: 76.510% (7651/10000 100/100 ======>........................................................]  Step: None | Tot: None | Loss: 0.683 | Acc: 76.643% (1073/1400 14/100 ================>................................................]  Step: None | Tot: None | Loss: 0.724 | Acc: 76.115% (1979/2600 26/100 =================>...............................................]  Step: None | Tot: None | Loss: 0.714 | Acc: 76.500% (2142/2800 28/100 ========================>........................................]  Step: None | Tot: None | Loss: 0.714 | Acc: 76.718% (2992/3900 39/100 =======================================>.........................]  Step: None | Tot: None | Loss: 0.709 | Acc: 76.623% (4674/6100 61/100 ==================================================>..............]  Step: None | Tot: None | Loss: 0.701 | Acc: 76.705% (5983/7800 78/100 =====================================================>...........]  Step: None | Tot: None | Loss: 0.699 | Acc: 76.735% (6369/8300 83/100 ======================================================>..........]  Step: None | Tot: None | Loss: 0.700 | Acc: 76.647% (6515/8500 85/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 5\n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.562 | Acc: 80.688% (40344/50000 391/391 \n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.612 | Acc: 78.910% (7891/10000 100/100 ===================>...........................................]  Step: None | Tot: None | Loss: 0.611 | Acc: 78.588% (2672/3400 34/100 ==============================>..................................]  Step: None | Tot: None | Loss: 0.610 | Acc: 78.917% (3788/4800 48/100 ==================================>..............................]  Step: None | Tot: None | Loss: 0.609 | Acc: 78.926% (4262/5400 54/100 ====================================>............................]  Step: None | Tot: None | Loss: 0.610 | Acc: 78.965% (4501/5700 57/100 ========================================================>........]  Step: None | Tot: None | Loss: 0.611 | Acc: 78.830% (6937/8800 88/100 ===========================================================>.....]  Step: None | Tot: None | Loss: 0.610 | Acc: 78.870% (7256/9200 92/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 6\n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.513 | Acc: 82.334% (41167/50000 391/391 ===========================>.....................................]  Step: None | Tot: None | Loss: 0.511 | Acc: 82.222% (17260/20992 164/391 \n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.585 | Acc: 80.680% (8068/10000 100/100 ========>......................................................]  Step: None | Tot: None | Loss: 0.568 | Acc: 80.765% (1373/1700 17/100 =============>...................................................]  Step: None | Tot: None | Loss: 0.579 | Acc: 80.000% (1760/2200 22/100 ================>................................................]  Step: None | Tot: None | Loss: 0.591 | Acc: 79.808% (2075/2600 26/100 ========================>........................................]  Step: None | Tot: None | Loss: 0.595 | Acc: 80.211% (3048/3800 38/100 ===============================================>.................]  Step: None | Tot: None | Loss: 0.585 | Acc: 80.824% (5981/7400 74/100 ==================================================>..............]  Step: None | Tot: None | Loss: 0.585 | Acc: 80.808% (6303/7800 78/100 ===========================================================>.....]  Step: None | Tot: None | Loss: 0.582 | Acc: 80.826% (7436/9200 92/100 =============================================================>...]  Step: None | Tot: None | Loss: 0.583 | Acc: 80.792% (7756/9600 96/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 7\n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.469 | Acc: 83.762% (41881/50000 391/391 ====================================================>............]  Step: None | Tot: None | Loss: 0.468 | Acc: 83.841% (34234/40832 319/391 \n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.573 | Acc: 81.550% (8155/10000 100/100 ============>..................................................]  Step: None | Tot: None | Loss: 0.566 | Acc: 81.652% (1878/2300 23/100 ==================>..............................................]  Step: None | Tot: None | Loss: 0.572 | Acc: 81.900% (2457/3000 30/100 =======================================>.........................]  Step: None | Tot: None | Loss: 0.573 | Acc: 81.820% (4991/6100 61/100 ========================================>........................]  Step: None | Tot: None | Loss: 0.569 | Acc: 81.844% (5238/6400 64/100 ==========================================>......................]  Step: None | Tot: None | Loss: 0.570 | Acc: 81.758% (5396/6600 66/100 ==================================================>..............]  Step: None | Tot: None | Loss: 0.574 | Acc: 81.654% (6369/7800 78/100 ==============================================================>..]  Step: None | Tot: None | Loss: 0.572 | Acc: 81.577% (7913/9700 97/100 ===============================================================>.]  Step: None | Tot: None | Loss: 0.573 | Acc: 81.571% (7994/9800 98/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 8\n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.429 | Acc: 85.048% (42524/50000 391/391 \n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.490 | Acc: 83.450% (8345/10000 100/100 ==>...........................................................]  Step: None | Tot: None | Loss: 0.505 | Acc: 82.778% (745/900)  9/100 =====>...........................................................]  Step: None | Tot: None | Loss: 0.498 | Acc: 82.900% (829/1000) 10/100 ===========>.....................................................]  Step: None | Tot: None | Loss: 0.485 | Acc: 83.167% (1497/1800 18/100 =============>...................................................]  Step: None | Tot: None | Loss: 0.494 | Acc: 82.667% (1736/2100 21/100 ========================>........................................]  Step: None | Tot: None | Loss: 0.502 | Acc: 83.079% (3157/3800 38/100 ==============================>..................................]  Step: None | Tot: None | Loss: 0.496 | Acc: 83.604% (4013/4800 48/100 ===============================>.................................]  Step: None | Tot: None | Loss: 0.494 | Acc: 83.694% (4101/4900 49/100 =================================>...............................]  Step: None | Tot: None | Loss: 0.493 | Acc: 83.635% (4349/5200 52/100 ============================================>....................]  Step: None | Tot: None | Loss: 0.498 | Acc: 83.348% (5751/6900 69/100 =======================================================>.........]  Step: None | Tot: None | Loss: 0.492 | Acc: 83.442% (7176/8600 86/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 9\n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.407 | Acc: 85.986% (42993/50000 391/391 \n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.630 | Acc: 80.570% (8057/10000 100/100 ===============================>...............................]  Step: None | Tot: None | Loss: 0.621 | Acc: 80.923% (4208/5200 52/100 ==================================>..............................]  Step: None | Tot: None | Loss: 0.625 | Acc: 80.704% (4358/5400 54/100 ===========================================>.....................]  Step: None | Tot: None | Loss: 0.629 | Acc: 80.588% (5480/6800 68/100 \n",
      "\n",
      "Epoch: 10\n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.374 | Acc: 87.046% (43523/50000 391/391 ====================>............................................]  Step: None | Tot: None | Loss: 0.370 | Acc: 87.437% (13878/15872 124/391 \n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.509 | Acc: 83.050% (8305/10000 100/100 =========>.....................................................]  Step: None | Tot: None | Loss: 0.469 | Acc: 84.111% (1514/1800 18/100 ==============>..................................................]  Step: None | Tot: None | Loss: 0.491 | Acc: 83.565% (1922/2300 23/100 ==============>..................................................]  Step: None | Tot: None | Loss: 0.490 | Acc: 83.625% (2007/2400 24/100 ====================>............................................]  Step: None | Tot: None | Loss: 0.506 | Acc: 83.594% (2675/3200 32/100 ======================>..........................................]  Step: None | Tot: None | Loss: 0.510 | Acc: 83.417% (3003/3600 36/100 ==========================>......................................]  Step: None | Tot: None | Loss: 0.513 | Acc: 83.073% (3406/4100 41/100 ==============================>..................................]  Step: None | Tot: None | Loss: 0.513 | Acc: 83.250% (3996/4800 48/100 ===============================>.................................]  Step: None | Tot: None | Loss: 0.510 | Acc: 83.327% (4083/4900 49/100 ===============================>.................................]  Step: None | Tot: None | Loss: 0.513 | Acc: 83.280% (4164/5000 50/100 =====================================================>...........]  Step: None | Tot: None | Loss: 0.512 | Acc: 82.940% (6967/8400 84/100 ===============================================================>.]  Step: None | Tot: None | Loss: 0.509 | Acc: 83.071% (8141/9800 98/100 \n",
      "\n",
      "Epoch: 11\n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.354 | Acc: 87.694% (43847/50000 391/391 \n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.484 | Acc: 84.270% (8427/10000 100/100 >.............................................................]  Step: None | Tot: None | Loss: 0.414 | Acc: 86.333% (518/600)  6/100 =====>...........................................................]  Step: None | Tot: None | Loss: 0.478 | Acc: 84.222% (758/900)  9/100 =====>...........................................................]  Step: None | Tot: None | Loss: 0.470 | Acc: 84.500% (845/1000) 10/100 =======>.........................................................]  Step: None | Tot: None | Loss: 0.472 | Acc: 84.462% (1098/1300 13/100 ====================>............................................]  Step: None | Tot: None | Loss: 0.488 | Acc: 84.000% (2772/3300 33/100 ======================>..........................................]  Step: None | Tot: None | Loss: 0.489 | Acc: 84.139% (3029/3600 36/100 ===============================>.................................]  Step: None | Tot: None | Loss: 0.485 | Acc: 84.360% (4218/5000 50/100 =======================================>.........................]  Step: None | Tot: None | Loss: 0.486 | Acc: 84.210% (5221/6200 62/100 ============================================>....................]  Step: None | Tot: None | Loss: 0.486 | Acc: 84.200% (5894/7000 70/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 12\n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.334 | Acc: 88.522% (44261/50000 391/391 ========================>........................................]  Step: None | Tot: None | Loss: 0.330 | Acc: 88.693% (17029/19200 150/391 \n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.461 | Acc: 85.150% (8515/10000 100/100 =>............................................................]  Step: None | Tot: None | Loss: 0.439 | Acc: 84.750% (678/800)  8/100 ========>........................................................]  Step: None | Tot: None | Loss: 0.441 | Acc: 85.571% (1198/1400 14/100 =============================>...................................]  Step: None | Tot: None | Loss: 0.459 | Acc: 85.043% (3912/4600 46/100 ========================================>........................]  Step: None | Tot: None | Loss: 0.460 | Acc: 85.141% (5449/6400 64/100 ==========================================>......................]  Step: None | Tot: None | Loss: 0.461 | Acc: 85.090% (5701/6700 67/100 =========================================================>.......]  Step: None | Tot: None | Loss: 0.463 | Acc: 85.112% (7575/8900 89/100 ===========================================================>.....]  Step: None | Tot: None | Loss: 0.462 | Acc: 85.151% (7919/9300 93/100 ============================================================>....]  Step: None | Tot: None | Loss: 0.462 | Acc: 85.138% (8003/9400 94/100 =============================================================>...]  Step: None | Tot: None | Loss: 0.461 | Acc: 85.158% (8090/9500 95/100 ===============================================================>.]  Step: None | Tot: None | Loss: 0.463 | Acc: 85.091% (8424/9900 99/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 13\n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.318 | Acc: 88.934% (44467/50000 391/391 \n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.492 | Acc: 84.280% (8428/10000 100/100 ==>...........................................................]  Step: None | Tot: None | Loss: 0.438 | Acc: 86.100% (861/1000) 10/100 ============>....................................................]  Step: None | Tot: None | Loss: 0.487 | Acc: 85.200% (1704/2000 20/100 ====================>............................................]  Step: None | Tot: None | Loss: 0.490 | Acc: 84.844% (2715/3200 32/100 ========================>........................................]  Step: None | Tot: None | Loss: 0.499 | Acc: 84.474% (3210/3800 38/100 ==============================>..................................]  Step: None | Tot: None | Loss: 0.489 | Acc: 84.646% (4063/4800 48/100 ===========================================>.....................]  Step: None | Tot: None | Loss: 0.495 | Acc: 84.500% (5746/6800 68/100 =====================================================>...........]  Step: None | Tot: None | Loss: 0.494 | Acc: 84.286% (7080/8400 84/100 =======================================================>.........]  Step: None | Tot: None | Loss: 0.496 | Acc: 84.253% (7330/8700 87/100 =========================================================>.......]  Step: None | Tot: None | Loss: 0.496 | Acc: 84.211% (7579/9000 90/100 ==========================================================>......]  Step: None | Tot: None | Loss: 0.494 | Acc: 84.275% (7669/9100 91/100 \n",
      "\n",
      "Epoch: 14\n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.299 | Acc: 89.684% (44842/50000 391/391 \n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.452 | Acc: 85.680% (8568/10000 100/100 =====>.........................................................]  Step: None | Tot: None | Loss: 0.427 | Acc: 86.083% (1033/1200 12/100 ===========>.....................................................]  Step: None | Tot: None | Loss: 0.431 | Acc: 85.833% (1545/1800 18/100 ==============>..................................................]  Step: None | Tot: None | Loss: 0.470 | Acc: 85.087% (1957/2300 23/100 ========================>........................................]  Step: None | Tot: None | Loss: 0.483 | Acc: 85.474% (3248/3800 38/100 ==========================>......................................]  Step: None | Tot: None | Loss: 0.469 | Acc: 85.833% (3605/4200 42/100 ==========================================>......................]  Step: None | Tot: None | Loss: 0.463 | Acc: 85.621% (5651/6600 66/100 ================================================>................]  Step: None | Tot: None | Loss: 0.455 | Acc: 85.733% (6430/7500 75/100 ==================================================>..............]  Step: None | Tot: None | Loss: 0.454 | Acc: 85.782% (6691/7800 78/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 15\n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.280 | Acc: 90.140% (45070/50000 391/391 \n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.447 | Acc: 85.750% (8575/10000 100/100 >.............................................................]  Step: None | Tot: None | Loss: 0.348 | Acc: 87.667% (526/600)  6/100 =========>.......................................................]  Step: None | Tot: None | Loss: 0.396 | Acc: 86.875% (1390/1600 16/100 ==============>..................................................]  Step: None | Tot: None | Loss: 0.439 | Acc: 86.250% (2070/2400 24/100 ======================>..........................................]  Step: None | Tot: None | Loss: 0.449 | Acc: 85.639% (3083/3600 36/100 ==========================>......................................]  Step: None | Tot: None | Loss: 0.453 | Acc: 85.585% (3509/4100 41/100 ===========================>.....................................]  Step: None | Tot: None | Loss: 0.447 | Acc: 85.841% (3777/4400 44/100 ============================>....................................]  Step: None | Tot: None | Loss: 0.447 | Acc: 85.889% (3865/4500 45/100 =============================>...................................]  Step: None | Tot: None | Loss: 0.448 | Acc: 85.848% (3949/4600 46/100 ===============================================>.................]  Step: None | Tot: None | Loss: 0.451 | Acc: 85.608% (6335/7400 74/100 ===========================================================>.....]  Step: None | Tot: None | Loss: 0.447 | Acc: 85.785% (7978/9300 93/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 16\n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.268 | Acc: 90.712% (45356/50000 391/391 \n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.466 | Acc: 85.480% (8548/10000 100/100 ==>...........................................................]  Step: None | Tot: None | Loss: 0.409 | Acc: 85.778% (772/900)  9/100 ============>....................................................]  Step: None | Tot: None | Loss: 0.474 | Acc: 85.550% (1711/2000 20/100 =============>...................................................]  Step: None | Tot: None | Loss: 0.488 | Acc: 85.227% (1875/2200 22/100 ====================================>............................]  Step: None | Tot: None | Loss: 0.484 | Acc: 85.386% (4867/5700 57/100 ===============================================================>.]  Step: None | Tot: None | Loss: 0.466 | Acc: 85.520% (8381/9800 98/100 \n",
      "\n",
      "Epoch: 17\n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.262 | Acc: 90.968% (45484/50000 391/391 \n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.562 | Acc: 83.170% (8317/10000 100/100 ==============>................................................]  Step: None | Tot: None | Loss: 0.582 | Acc: 82.923% (2156/2600 26/100 =======================>.........................................]  Step: None | Tot: None | Loss: 0.568 | Acc: 83.297% (3082/3700 37/100 =====================================>...........................]  Step: None | Tot: None | Loss: 0.566 | Acc: 82.931% (4810/5800 58/100 =========================================================>.......]  Step: None | Tot: None | Loss: 0.564 | Acc: 83.044% (7474/9000 90/100 \n",
      "\n",
      "Epoch: 18\n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.247 | Acc: 91.446% (45723/50000 391/391 ==============================>..................................]  Step: None | Tot: None | Loss: 0.243 | Acc: 91.602% (21574/23552 184/391 =======================================>.........................]  Step: None | Tot: None | Loss: 0.242 | Acc: 91.576% (28132/30720 240/391 \n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.406 | Acc: 86.990% (8699/10000 100/100 =========>.....................................................]  Step: None | Tot: None | Loss: 0.419 | Acc: 86.611% (1559/1800 18/100 ============>....................................................]  Step: None | Tot: None | Loss: 0.434 | Acc: 86.250% (1725/2000 20/100 ====================>............................................]  Step: None | Tot: None | Loss: 0.426 | Acc: 86.625% (2772/3200 32/100 ===============================>.................................]  Step: None | Tot: None | Loss: 0.415 | Acc: 86.880% (4344/5000 50/100 ======================================>..........................]  Step: None | Tot: None | Loss: 0.406 | Acc: 86.967% (5218/6000 60/100 =======================================>.........................]  Step: None | Tot: None | Loss: 0.407 | Acc: 86.887% (5387/6200 62/100 ========================================>........................]  Step: None | Tot: None | Loss: 0.407 | Acc: 86.905% (5475/6300 63/100 ============================================>....................]  Step: None | Tot: None | Loss: 0.406 | Acc: 86.929% (6085/7000 70/100 ==============================================>..................]  Step: None | Tot: None | Loss: 0.405 | Acc: 86.944% (6260/7200 72/100 =====================================================>...........]  Step: None | Tot: None | Loss: 0.406 | Acc: 86.917% (7301/8400 84/100 ===============================================================>.]  Step: None | Tot: None | Loss: 0.406 | Acc: 86.990% (8525/9800 98/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 19\n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.234 | Acc: 91.728% (45864/50000 391/391 \n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.382 | Acc: 87.570% (8757/10000 100/100 >.............................................................]  Step: None | Tot: None | Loss: 0.289 | Acc: 90.500% (543/600)  6/100 ==========>......................................................]  Step: None | Tot: None | Loss: 0.359 | Acc: 88.529% (1505/1700 17/100 ==================>..............................................]  Step: None | Tot: None | Loss: 0.395 | Acc: 87.100% (2613/3000 30/100 ===========================>.....................................]  Step: None | Tot: None | Loss: 0.387 | Acc: 87.535% (3764/4300 43/100 ======================================================>..........]  Step: None | Tot: None | Loss: 0.379 | Acc: 87.600% (7446/8500 85/100 ============================================================>....]  Step: None | Tot: None | Loss: 0.383 | Acc: 87.532% (8228/9400 94/100 =============================================================>...]  Step: None | Tot: None | Loss: 0.383 | Acc: 87.495% (8312/9500 95/100 \n",
      "Saving..\n",
      "773.4955949783325\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Training\n",
    "def modelpar_train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to('cuda:0'), targets.to('cuda:1')\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "        \n",
    "def modelpar_test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to('cuda:0'), targets.to('cuda:1')\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/ckpt.pth')\n",
    "        best_acc = acc\n",
    "\n",
    "t = time.time()\n",
    "for epoch in range(start_epoch, start_epoch+20):\n",
    "    modelpar_train(epoch)\n",
    "    modelpar_test(epoch)\n",
    "    scheduler.step()\n",
    "print(time.time() - t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8f1606-c622-4c94-90ac-42a109e2b737",
   "metadata": {},
   "source": [
    "Model parallism 20 epochs - time-773.4955949783325s 91 training and 87 testing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b66b7e8f-0a2f-4422-8e59-b55393140c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineParallelResNet50(ResNet):\n",
    "    def __init__(self, split_size=20, *args, **kwargs):\n",
    "        super(PipelineParallelResNet50, self).__init__(*args, **kwargs)\n",
    "        self.split_size = split_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        splits = iter(x.split(self.split_size, dim=0))\n",
    "        s_next = next(splits)\n",
    "        s_prev = self.seq1(s_next).to('cuda:1')\n",
    "        ret = []\n",
    "\n",
    "        for s_next in splits:\n",
    "            # A. s_prev runs on cuda:1\n",
    "            s_prev = self.seq2(s_prev)\n",
    "            ret.append(self.fc(s_prev.view(s_prev.size(0), -1)))\n",
    "\n",
    "            # B. s_next runs on cuda:0, which can run concurrently with A\n",
    "            s_prev = self.seq1(s_next).to('cuda:1')\n",
    "\n",
    "        s_prev = self.seq2(s_prev)\n",
    "        ret.append(self.fc(s_prev.view(s_prev.size(0), -1)))\n",
    "\n",
    "        return torch.cat(ret)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fe02bf8-cfbf-41ad-9307-1b4f8310a550",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'block' and 'num_blocks'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m num_repeat \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      7\u001b[0m setup \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel = PipelineParallelResNet50()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m pp_run_times \u001b[38;5;241m=\u001b[39m \u001b[43mtimeit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msetup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepeat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_repeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m pp_mean, pp_std \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(pp_run_times), np\u001b[38;5;241m.\u001b[39mstd(pp_run_times)\n\u001b[1;32m     12\u001b[0m plt\u001b[38;5;241m.\u001b[39merrorbar([pp_mean], [pp_std], fmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPipelining Model Parallel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/packages/7x/jupyter/2022-04-15/lib/python3.9/timeit.py:238\u001b[0m, in \u001b[0;36mrepeat\u001b[0;34m(stmt, setup, timer, repeat, number, globals)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrepeat\u001b[39m(stmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass\u001b[39m\u001b[38;5;124m\"\u001b[39m, setup\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass\u001b[39m\u001b[38;5;124m\"\u001b[39m, timer\u001b[38;5;241m=\u001b[39mdefault_timer,\n\u001b[1;32m    236\u001b[0m            repeat\u001b[38;5;241m=\u001b[39mdefault_repeat, number\u001b[38;5;241m=\u001b[39mdefault_number, \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;124;03m\"\"\"Convenience function to create Timer object and call repeat method.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTimer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msetup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/packages/7x/jupyter/2022-04-15/lib/python3.9/timeit.py:205\u001b[0m, in \u001b[0;36mTimer.repeat\u001b[0;34m(self, repeat, number)\u001b[0m\n\u001b[1;32m    203\u001b[0m r \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(repeat):\n\u001b[0;32m--> 205\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     r\u001b[38;5;241m.\u001b[39mappend(t)\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m/packages/7x/jupyter/2022-04-15/lib/python3.9/timeit.py:177\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    175\u001b[0m gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     timing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gcold:\n",
      "File \u001b[0;32m<timeit-src>:3\u001b[0m, in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mPipelineParallelResNet50.__init__\u001b[0;34m(self, split_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, split_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mPipelineParallelResNet50\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_size \u001b[38;5;241m=\u001b[39m split_size\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'block' and 'num_blocks'"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "stmt = \"train(model)\"\n",
    "num_repeat = 2\n",
    "setup = \"model = PipelineParallelResNet50()\"\n",
    "pp_run_times = timeit.repeat(\n",
    "    stmt, setup, number=1, repeat=num_repeat, globals=globals())\n",
    "pp_mean, pp_std = np.mean(pp_run_times), np.std(pp_run_times)\n",
    "\n",
    "plt.errorbar([pp_mean], [pp_std], fmt='o', label='Pipelining Model Parallel')\n",
    "plt.legend()\n",
    "plt.savefig('pp.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350f1963-ea2b-4ddd-b2f0-8ed0f5d2d569",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
