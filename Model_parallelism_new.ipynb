{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "216c61cb-aca7-45d0-8bff-e550af1a2ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchvision in ./.local/lib/python3.9/site-packages (0.15.1)\n",
      "Requirement already satisfied: torch==2.0.0 in ./.local/lib/python3.9/site-packages (from torchvision) (2.0.0)\n",
      "Requirement already satisfied: numpy in /packages/7x/jupyter/2022-04-15/lib/python3.9/site-packages (from torchvision) (1.22.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /packages/7x/jupyter/2022-04-15/lib/python3.9/site-packages (from torchvision) (9.1.0)\n",
      "Requirement already satisfied: requests in /packages/7x/jupyter/2022-04-15/lib/python3.9/site-packages (from torchvision) (2.27.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in ./.local/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in ./.local/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (11.7.4.91)\n",
      "Requirement already satisfied: jinja2 in /packages/7x/jupyter/2022-04-15/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (3.1.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./.local/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./.local/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in ./.local/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (11.7.101)\n",
      "Requirement already satisfied: sympy in ./.local/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (1.11.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./.local/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./.local/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in ./.local/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (11.4.0.1)\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (3.1)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in ./.local/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (10.2.10.91)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (3.11.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in ./.local/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (10.9.0.58)\n",
      "Requirement already satisfied: triton==2.0.0 in ./.local/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (2.0.0)\n",
      "Requirement already satisfied: typing-extensions in ./.local/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (4.5.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in ./.local/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (11.7.91)\n",
      "Requirement already satisfied: setuptools in /packages/7x/jupyter/2022-04-15/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchvision) (58.0.4)\n",
      "Requirement already satisfied: wheel in /packages/7x/jupyter/2022-04-15/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchvision) (0.37.1)\n",
      "Requirement already satisfied: lit in ./.local/lib/python3.9/site-packages (from triton==2.0.0->torch==2.0.0->torchvision) (16.0.1)\n",
      "Requirement already satisfied: cmake in ./.local/lib/python3.9/site-packages (from triton==2.0.0->torch==2.0.0->torchvision) (3.26.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /packages/7x/jupyter/2022-04-15/lib/python3.9/site-packages (from jinja2->torch==2.0.0->torchvision) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /packages/7x/jupyter/2022-04-15/lib/python3.9/site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /packages/7x/jupyter/2022-04-15/lib/python3.9/site-packages (from requests->torchvision) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /packages/7x/jupyter/2022-04-15/lib/python3.9/site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /packages/7x/jupyter/2022-04-15/lib/python3.9/site-packages (from requests->torchvision) (2021.10.8)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.local/lib/python3.9/site-packages (from sympy->torch==2.0.0->torchvision) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6286473-461d-433c-bf59-efaf4ea11b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import torch.distributed as dist\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        \n",
    "       \n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        if dist.is_initialized():\n",
    "            self.conv1 = nn.parallel.DistributedDataParallel(self.conv1)\n",
    "            self.bn1 = nn.parallel.DistributedDataParallel(self.bn1)\n",
    "        \n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "        \n",
    "        # Wrap the layers with the DataParallel module to parallelize the computation.\n",
    "        if dist.is_initialized():\n",
    "            self.layer1 = nn.parallel.DistributedDataParallel(self.layer1)\n",
    "            self.layer2 = nn.parallel.DistributedDataParallel(self.layer2)\n",
    "            self.layer3 = nn.parallel.DistributedDataParallel(self.layer3)\n",
    "            self.layer4 = nn.parallel.DistributedDataParallel(self.layer4)\n",
    "            self.linear = nn.parallel.DistributedDataParallel(self.linear)\n",
    "        \n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "    \n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
    "\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
    "\n",
    "\n",
    "def test():\n",
    "    net = ResNet18()\n",
    "    y = net(torch.randn(1, 3, 32, 32))\n",
    "    print(y.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d20d57ef-ef6f-4a23-9362-e4537f3af438",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Some helper functions for PyTorch, including:\n",
    "    - get_mean_and_std: calculate the mean and std value of dataset.\n",
    "    - msr_init: net parameter initialization.\n",
    "    - progress_bar: progress bar mimic xlua.progress.\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import time\n",
    "\n",
    "\n",
    "def get_mean_and_std(dataset):\n",
    "    '''Compute the mean and std value of dataset.'''\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n",
    "    mean = torch.zeros(3)\n",
    "    std = torch.zeros(3)\n",
    "    print('==> Computing mean and std..')\n",
    "    for inputs, targets in dataloader:\n",
    "        for i in range(3):\n",
    "            mean[i] += inputs[:,i,:,:].mean()\n",
    "            std[i] += inputs[:,i,:,:].std()\n",
    "    mean.div_(len(dataset))\n",
    "    std.div_(len(dataset))\n",
    "    return mean, std\n",
    "\n",
    "def init_params(net):\n",
    "    '''Init layer parameters.'''\n",
    "    for m in net.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "            if m.bias is not None:\n",
    "                init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            init.constant_(m.weight, 1)\n",
    "            init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            init.normal_(m.weight, std=1e-3)\n",
    "            if m.bias is not None:\n",
    "                init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "TOTAL_BAR_LENGTH = 65.\n",
    "last_time = time.time()\n",
    "begin_time = last_time\n",
    "def progress_bar(current, total, msg=None):\n",
    "    global last_time, begin_time\n",
    "    if current == 0:\n",
    "        begin_time = time.time()  # Reset for new bar.\n",
    "\n",
    "    cur_len = int(TOTAL_BAR_LENGTH*current/total)\n",
    "    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n",
    "\n",
    "    sys.stdout.write(' [')\n",
    "    for i in range(cur_len):\n",
    "        sys.stdout.write('=')\n",
    "    sys.stdout.write('>')\n",
    "    for i in range(rest_len):\n",
    "        sys.stdout.write('.')\n",
    "    sys.stdout.write(']')\n",
    "\n",
    "    cur_time = time.time()\n",
    "    step_time = cur_time - last_time\n",
    "    last_time = cur_time\n",
    "    tot_time = cur_time - begin_time\n",
    "\n",
    "    L = []\n",
    "    L.append('  Step: %s' % format_time(step_time))\n",
    "    L.append(' | Tot: %s' % format_time(tot_time))\n",
    "    if msg:\n",
    "        L.append(' | ' + msg)\n",
    "\n",
    "    msg = ''.join(L)\n",
    "    sys.stdout.write(msg)\n",
    "    for i in range(int(TOTAL_BAR_LENGTH) - len(msg)):\n",
    "        sys.stdout.write(' ')\n",
    "\n",
    "    # Go back to the center of the bar.\n",
    "    for i in range(int(TOTAL_BAR_LENGTH/2) + 7):\n",
    "        sys.stdout.write('\\b')\n",
    "    sys.stdout.write(' %d/%d ' % (current+1, total))\n",
    "\n",
    "    if current < total-1:\n",
    "        sys.stdout.write('\\r')\n",
    "    else:\n",
    "        sys.stdout.write('\\n')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def format_time(seconds):\n",
    "    days = int(seconds / 3600/24)\n",
    "    seconds = seconds - days*3600*24\n",
    "    hours = int(seconds / 3600)\n",
    "    seconds = seconds - hours*3600\n",
    "    minutes = int(seconds / 60)\n",
    "    seconds = seconds - minutes*60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cdbaaa4-a495-4866-a628-fb64aa0683d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "==> Building model..\n",
      "\n",
      "Epoch: 0\n",
      " [================================================================>]  Step: None | Tot: None | Loss: 2.510 | Acc: 20.772% (10386/50000 391/391 \n",
      " [================================================================>]  Step: None | Tot: None | Loss: 1.889 | Acc: 31.020% (3102/10000 100/100 ===========>...................................................]  Step: None | Tot: None | Loss: 1.915 | Acc: 30.409% (669/2200) 22/100 =================>...............................................]  Step: None | Tot: None | Loss: 1.915 | Acc: 30.393% (851/2800) 28/100 ===================>.............................................]  Step: None | Tot: None | Loss: 1.909 | Acc: 30.613% (949/3100) 31/100 =====================>...........................................]  Step: None | Tot: None | Loss: 1.907 | Acc: 30.706% (1044/3400 34/100 =========================>.......................................]  Step: None | Tot: None | Loss: 1.907 | Acc: 30.925% (1237/4000 40/100 ===========================>.....................................]  Step: None | Tot: None | Loss: 1.902 | Acc: 31.205% (1373/4400 44/100 ====================================>............................]  Step: None | Tot: None | Loss: 1.900 | Acc: 31.211% (1779/5700 57/100 ==========================================>......................]  Step: None | Tot: None | Loss: 1.900 | Acc: 30.851% (2067/6700 67/100 ===============================================>.................]  Step: None | Tot: None | Loss: 1.898 | Acc: 30.703% (2272/7400 74/100 =================================================>...............]  Step: None | Tot: None | Loss: 1.898 | Acc: 30.831% (2374/7700 77/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 1\n",
      " [================================================================>]  Step: None | Tot: None | Loss: 1.793 | Acc: 33.876% (16938/50000 391/391 \n",
      " [================================================================>]  Step: None | Tot: None | Loss: 1.615 | Acc: 40.360% (4036/10000 100/100 ========>......................................................]  Step: None | Tot: None | Loss: 1.596 | Acc: 41.529% (706/1700) 17/100 ================>................................................]  Step: None | Tot: None | Loss: 1.608 | Acc: 41.077% (1068/2600 26/100 =================================================>...............]  Step: None | Tot: None | Loss: 1.611 | Acc: 40.364% (3108/7700 77/100 ===================================================>.............]  Step: None | Tot: None | Loss: 1.613 | Acc: 40.237% (3219/8000 80/100 ===========================================================>.....]  Step: None | Tot: None | Loss: 1.615 | Acc: 40.283% (3706/9200 92/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 2\n",
      " [================================================================>]  Step: None | Tot: None | Loss: 1.604 | Acc: 40.882% (20441/50000 391/391 \n",
      " [================================================================>]  Step: None | Tot: None | Loss: 1.537 | Acc: 44.120% (4412/10000 100/100 ==>...........................................................]  Step: None | Tot: None | Loss: 1.504 | Acc: 45.333% (408/900)  9/100 =====>...........................................................]  Step: None | Tot: None | Loss: 1.487 | Acc: 46.300% (463/1000) 10/100 ======>..........................................................]  Step: None | Tot: None | Loss: 1.481 | Acc: 46.727% (514/1100) 11/100 ===========>.....................................................]  Step: None | Tot: None | Loss: 1.529 | Acc: 45.105% (857/1900) 19/100 ===================>.............................................]  Step: None | Tot: None | Loss: 1.536 | Acc: 45.032% (1396/3100 31/100 =============================>...................................]  Step: None | Tot: None | Loss: 1.527 | Acc: 44.870% (2064/4600 46/100 ==================================================>..............]  Step: None | Tot: None | Loss: 1.544 | Acc: 44.013% (3477/7900 79/100 ===========================================================>.....]  Step: None | Tot: None | Loss: 1.539 | Acc: 44.196% (4066/9200 92/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 3\n",
      " [================================================================>]  Step: None | Tot: None | Loss: 1.431 | Acc: 47.396% (23698/50000 391/391 \n",
      " [================================================================>]  Step: None | Tot: None | Loss: 1.437 | Acc: 48.350% (4835/10000 100/100 >.............................................................]  Step: None | Tot: None | Loss: 1.406 | Acc: 48.714% (341/700)  7/100 =============>...................................................]  Step: None | Tot: None | Loss: 1.428 | Acc: 49.273% (1084/2200 22/100 =========================>.......................................]  Step: None | Tot: None | Loss: 1.434 | Acc: 48.725% (1949/4000 40/100 ==============================================>..................]  Step: None | Tot: None | Loss: 1.439 | Acc: 48.384% (3532/7300 73/100 =================================================>...............]  Step: None | Tot: None | Loss: 1.437 | Acc: 48.377% (3725/7700 77/100 =============================================================>...]  Step: None | Tot: None | Loss: 1.436 | Acc: 48.448% (4651/9600 96/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 4\n",
      " [================================================================>]  Step: None | Tot: None | Loss: 1.254 | Acc: 54.828% (27414/50000 391/391 \n",
      " [================================================================>]  Step: None | Tot: None | Loss: 1.200 | Acc: 56.930% (5693/10000 100/100 =====>.........................................................]  Step: None | Tot: None | Loss: 1.173 | Acc: 56.750% (681/1200) 12/100 ==========>......................................................]  Step: None | Tot: None | Loss: 1.200 | Acc: 57.059% (970/1700) 17/100 =====================>...........................................]  Step: None | Tot: None | Loss: 1.207 | Acc: 56.588% (1924/3400 34/100 ===============================>.................................]  Step: None | Tot: None | Loss: 1.200 | Acc: 56.878% (2787/4900 49/100 =====================================>...........................]  Step: None | Tot: None | Loss: 1.206 | Acc: 57.052% (3309/5800 58/100 ======================================>..........................]  Step: None | Tot: None | Loss: 1.210 | Acc: 56.933% (3416/6000 60/100 =======================================>.........................]  Step: None | Tot: None | Loss: 1.209 | Acc: 56.984% (3533/6200 62/100 ===========================================>.....................]  Step: None | Tot: None | Loss: 1.200 | Acc: 57.206% (3890/6800 68/100 ====================================================>............]  Step: None | Tot: None | Loss: 1.204 | Acc: 56.915% (4667/8200 82/100 ======================================================>..........]  Step: None | Tot: None | Loss: 1.206 | Acc: 56.800% (4828/8500 85/100 =============================================================>...]  Step: None | Tot: None | Loss: 1.201 | Acc: 56.895% (5405/9500 95/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 5\n",
      " [================================================================>]  Step: None | Tot: None | Loss: 1.110 | Acc: 60.274% (30137/50000 391/391 \n",
      " [================================================================>]  Step: None | Tot: None | Loss: 1.078 | Acc: 61.010% (6101/10000 100/100 ==================>............................................]  Step: None | Tot: None | Loss: 1.070 | Acc: 61.094% (1955/3200 32/100 =======================>.........................................]  Step: None | Tot: None | Loss: 1.082 | Acc: 60.703% (2246/3700 37/100 ============================>....................................]  Step: None | Tot: None | Loss: 1.073 | Acc: 61.378% (2762/4500 45/100 =============================>...................................]  Step: None | Tot: None | Loss: 1.070 | Acc: 61.532% (2892/4700 47/100 ===============================>.................................]  Step: None | Tot: None | Loss: 1.067 | Acc: 61.480% (3074/5000 50/100 ===================================>.............................]  Step: None | Tot: None | Loss: 1.070 | Acc: 61.446% (3441/5600 56/100 =====================================>...........................]  Step: None | Tot: None | Loss: 1.073 | Acc: 61.322% (3618/5900 59/100 ===========================================>.....................]  Step: None | Tot: None | Loss: 1.077 | Acc: 61.029% (4150/6800 68/100 ============================================>....................]  Step: None | Tot: None | Loss: 1.078 | Acc: 60.957% (4267/7000 70/100 ================================================>................]  Step: None | Tot: None | Loss: 1.078 | Acc: 60.973% (4573/7500 75/100 ====================================================>............]  Step: None | Tot: None | Loss: 1.080 | Acc: 61.000% (4941/8100 81/100 =========================================================>.......]  Step: None | Tot: None | Loss: 1.083 | Acc: 60.889% (5480/9000 90/100 ==========================================================>......]  Step: None | Tot: None | Loss: 1.081 | Acc: 60.890% (5541/9100 91/100 =============================================================>...]  Step: None | Tot: None | Loss: 1.077 | Acc: 61.063% (5801/9500 95/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 6\n",
      " [================================================================>]  Step: None | Tot: None | Loss: 1.000 | Acc: 64.514% (32257/50000 391/391 \n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.974 | Acc: 66.210% (6621/10000 100/100 =====>.........................................................]  Step: None | Tot: None | Loss: 0.952 | Acc: 66.231% (861/1300) 13/100 ============>....................................................]  Step: None | Tot: None | Loss: 0.977 | Acc: 65.650% (1313/2000 20/100 =============>...................................................]  Step: None | Tot: None | Loss: 0.967 | Acc: 65.905% (1384/2100 21/100 ===========================>.....................................]  Step: None | Tot: None | Loss: 0.972 | Acc: 66.395% (2855/4300 43/100 =================================================>...............]  Step: None | Tot: None | Loss: 0.970 | Acc: 66.351% (5109/7700 77/100 =========================================================>.......]  Step: None | Tot: None | Loss: 0.975 | Acc: 66.270% (5898/8900 89/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 7\n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.903 | Acc: 67.916% (33958/50000 391/391 \n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.968 | Acc: 65.970% (6597/10000 100/100 ===============>...............................................]  Step: None | Tot: None | Loss: 0.974 | Acc: 65.214% (1826/2800 28/100 ===================>.............................................]  Step: None | Tot: None | Loss: 0.970 | Acc: 65.452% (2029/3100 31/100 ====================>............................................]  Step: None | Tot: None | Loss: 0.967 | Acc: 65.500% (2096/3200 32/100 =====================>...........................................]  Step: None | Tot: None | Loss: 0.974 | Acc: 65.294% (2220/3400 34/100 =====================================>...........................]  Step: None | Tot: None | Loss: 0.967 | Acc: 65.793% (3816/5800 58/100 ==========================================>......................]  Step: None | Tot: None | Loss: 0.968 | Acc: 65.806% (4409/6700 67/100 ======================================================>..........]  Step: None | Tot: None | Loss: 0.968 | Acc: 66.071% (5616/8500 85/100 \n",
      "\n",
      "Epoch: 8\n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.833 | Acc: 70.462% (35231/50000 391/391 \n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.901 | Acc: 68.780% (6878/10000 100/100 ==>...........................................................]  Step: None | Tot: None | Loss: 0.905 | Acc: 67.667% (609/900)  9/100 ===========>.....................................................]  Step: None | Tot: None | Loss: 0.910 | Acc: 68.222% (1228/1800 18/100 =============>...................................................]  Step: None | Tot: None | Loss: 0.906 | Acc: 68.476% (1438/2100 21/100 ===========================>.....................................]  Step: None | Tot: None | Loss: 0.920 | Acc: 67.977% (2923/4300 43/100 ===============================>.................................]  Step: None | Tot: None | Loss: 0.910 | Acc: 68.490% (3356/4900 49/100 ============================================================>....]  Step: None | Tot: None | Loss: 0.901 | Acc: 68.904% (6477/9400 94/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 9\n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.778 | Acc: 72.582% (36291/50000 391/391 ================================================================>]  Step: None | Tot: None | Loss: 0.778 | Acc: 72.586% (36235/49920 390/391 \n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.851 | Acc: 69.390% (6939/10000 100/100 ===============>...............................................]  Step: None | Tot: None | Loss: 0.847 | Acc: 69.357% (1942/2800 28/100 =============================>...................................]  Step: None | Tot: None | Loss: 0.846 | Acc: 69.574% (3270/4700 47/100 =====================================>...........................]  Step: None | Tot: None | Loss: 0.845 | Acc: 69.712% (4113/5900 59/100 ==========================================================>......]  Step: None | Tot: None | Loss: 0.855 | Acc: 69.418% (6317/9100 91/100 ===========================================================>.....]  Step: None | Tot: None | Loss: 0.852 | Acc: 69.478% (6392/9200 92/100 =============================================================>...]  Step: None | Tot: None | Loss: 0.849 | Acc: 69.521% (6674/9600 96/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 10\n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.733 | Acc: 74.102% (37051/50000 391/391 ================================================================>]  Step: None | Tot: None | Loss: 0.733 | Acc: 74.092% (36797/49664 388/391 ================================================================>]  Step: None | Tot: None | Loss: 0.733 | Acc: 74.107% (36994/49920 390/391 \n",
      " [================================================================>]  Step: None | Tot: None | Loss: 1.022 | Acc: 65.210% (6521/10000 100/100 >.............................................................]  Step: None | Tot: None | Loss: 1.013 | Acc: 64.857% (454/700)  7/100 =========>.......................................................]  Step: None | Tot: None | Loss: 0.979 | Acc: 66.188% (1059/1600 16/100 ==============>..................................................]  Step: None | Tot: None | Loss: 1.007 | Acc: 65.542% (1573/2400 24/100 ===============>.................................................]  Step: None | Tot: None | Loss: 1.010 | Acc: 65.520% (1638/2500 25/100 ===========================================>.....................]  Step: None | Tot: None | Loss: 1.005 | Acc: 65.838% (4477/6800 68/100 =============================================>...................]  Step: None | Tot: None | Loss: 1.006 | Acc: 65.718% (4666/7100 71/100 \n",
      "\n",
      "Epoch: 11\n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.675 | Acc: 76.500% (38250/50000 391/391 \n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.671 | Acc: 76.570% (7657/10000 100/100 =====>.........................................................]  Step: None | Tot: None | Loss: 0.655 | Acc: 77.500% (930/1200) 12/100 ============>....................................................]  Step: None | Tot: None | Loss: 0.655 | Acc: 77.650% (1553/2000 20/100 ==================>..............................................]  Step: None | Tot: None | Loss: 0.669 | Acc: 77.267% (2318/3000 30/100 ======================>..........................................]  Step: None | Tot: None | Loss: 0.683 | Acc: 76.286% (2670/3500 35/100 ========================>........................................]  Step: None | Tot: None | Loss: 0.679 | Acc: 76.410% (2980/3900 39/100 ==================================================>..............]  Step: None | Tot: None | Loss: 0.671 | Acc: 76.456% (6040/7900 79/100 =========================================================>.......]  Step: None | Tot: None | Loss: 0.672 | Acc: 76.506% (6809/8900 89/100 ============================================================>....]  Step: None | Tot: None | Loss: 0.670 | Acc: 76.585% (7199/9400 94/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 12\n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.637 | Acc: 77.916% (38958/50000 391/391 \n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.663 | Acc: 76.940% (7694/10000 100/100 ===================>...........................................]  Step: None | Tot: None | Loss: 0.663 | Acc: 77.471% (2634/3400 34/100 ======================>..........................................]  Step: None | Tot: None | Loss: 0.669 | Acc: 77.257% (2704/3500 35/100 ========================================>........................]  Step: None | Tot: None | Loss: 0.665 | Acc: 77.016% (4929/6400 64/100 ==========================================>......................]  Step: None | Tot: None | Loss: 0.665 | Acc: 76.955% (5156/6700 67/100 ===========================================>.....................]  Step: None | Tot: None | Loss: 0.665 | Acc: 76.926% (5231/6800 68/100 ===============================================>.................]  Step: None | Tot: None | Loss: 0.664 | Acc: 76.973% (5696/7400 74/100 =======================================================>.........]  Step: None | Tot: None | Loss: 0.667 | Acc: 76.860% (6610/8600 86/100 ============================================================>....]  Step: None | Tot: None | Loss: 0.664 | Acc: 77.011% (7239/9400 94/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 13\n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.611 | Acc: 78.766% (39383/50000 391/391 \n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.785 | Acc: 73.490% (7349/10000 100/100 ==>...........................................................]  Step: None | Tot: None | Loss: 0.775 | Acc: 73.333% (660/900)  9/100 ==========>......................................................]  Step: None | Tot: None | Loss: 0.752 | Acc: 74.000% (1258/1700 17/100 ============>....................................................]  Step: None | Tot: None | Loss: 0.764 | Acc: 73.550% (1471/2000 20/100 ====================>............................................]  Step: None | Tot: None | Loss: 0.777 | Acc: 73.531% (2353/3200 32/100 ===========================>.....................................]  Step: None | Tot: None | Loss: 0.786 | Acc: 73.432% (3231/4400 44/100 =================================>...............................]  Step: None | Tot: None | Loss: 0.788 | Acc: 73.192% (3806/5200 52/100 ====================================>............................]  Step: None | Tot: None | Loss: 0.782 | Acc: 73.491% (4189/5700 57/100 =======================================>.........................]  Step: None | Tot: None | Loss: 0.778 | Acc: 73.581% (4562/6200 62/100 ================================================>................]  Step: None | Tot: None | Loss: 0.777 | Acc: 73.733% (5530/7500 75/100 ==================================================>..............]  Step: None | Tot: None | Loss: 0.778 | Acc: 73.679% (5747/7800 78/100 ===================================================>.............]  Step: None | Tot: None | Loss: 0.779 | Acc: 73.662% (5893/8000 80/100 ====================================================>............]  Step: None | Tot: None | Loss: 0.780 | Acc: 73.659% (6040/8200 82/100 =========================================================>.......]  Step: None | Tot: None | Loss: 0.788 | Acc: 73.622% (6626/9000 90/100 =============================================================>...]  Step: None | Tot: None | Loss: 0.784 | Acc: 73.632% (6995/9500 95/100 \n",
      "\n",
      "Epoch: 14\n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.591 | Acc: 79.466% (39733/50000 391/391 \n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.718 | Acc: 75.140% (7514/10000 100/100 =========>.....................................................]  Step: None | Tot: None | Loss: 0.703 | Acc: 74.333% (1338/1800 18/100 ==================>..............................................]  Step: None | Tot: None | Loss: 0.722 | Acc: 74.379% (2157/2900 29/100 ====================>............................................]  Step: None | Tot: None | Loss: 0.719 | Acc: 74.333% (2453/3300 33/100 =====================>...........................................]  Step: None | Tot: None | Loss: 0.719 | Acc: 74.294% (2526/3400 34/100 ======================>..........................................]  Step: None | Tot: None | Loss: 0.719 | Acc: 74.314% (2601/3500 35/100 ======================>..........................................]  Step: None | Tot: None | Loss: 0.720 | Acc: 74.361% (2677/3600 36/100 ========================>........................................]  Step: None | Tot: None | Loss: 0.724 | Acc: 74.342% (2825/3800 38/100 ==============================>..................................]  Step: None | Tot: None | Loss: 0.712 | Acc: 74.979% (3599/4800 48/100 ===============================>.................................]  Step: None | Tot: None | Loss: 0.707 | Acc: 75.143% (3682/4900 49/100 =====================================>...........................]  Step: None | Tot: None | Loss: 0.705 | Acc: 75.339% (4445/5900 59/100 ======================================>..........................]  Step: None | Tot: None | Loss: 0.704 | Acc: 75.317% (4519/6000 60/100 ==========================================>......................]  Step: None | Tot: None | Loss: 0.702 | Acc: 75.394% (4976/6600 66/100 ===========================================>.....................]  Step: None | Tot: None | Loss: 0.703 | Acc: 75.382% (5126/6800 68/100 =============================================>...................]  Step: None | Tot: None | Loss: 0.708 | Acc: 75.155% (5336/7100 71/100 ================================================>................]  Step: None | Tot: None | Loss: 0.711 | Acc: 75.240% (5643/7500 75/100 ================================================>................]  Step: None | Tot: None | Loss: 0.711 | Acc: 75.250% (5719/7600 76/100 =======================================================>.........]  Step: None | Tot: None | Loss: 0.717 | Acc: 75.116% (6460/8600 86/100 ===========================================================>.....]  Step: None | Tot: None | Loss: 0.717 | Acc: 75.130% (6912/9200 92/100 =============================================================>...]  Step: None | Tot: None | Loss: 0.717 | Acc: 75.198% (7219/9600 96/100 \n",
      "\n",
      "Epoch: 15\n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.571 | Acc: 80.420% (40210/50000 391/391 \n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.709 | Acc: 76.130% (7613/10000 100/100 ====>..........................................................]  Step: None | Tot: None | Loss: 0.716 | Acc: 75.273% (828/1100) 11/100 =======>.........................................................]  Step: None | Tot: None | Loss: 0.730 | Acc: 75.250% (903/1200) 12/100 ==========>......................................................]  Step: None | Tot: None | Loss: 0.711 | Acc: 75.882% (1290/1700 17/100 ==============>..................................................]  Step: None | Tot: None | Loss: 0.732 | Acc: 75.130% (1728/2300 23/100 ================>................................................]  Step: None | Tot: None | Loss: 0.730 | Acc: 75.519% (2039/2700 27/100 =======================>.........................................]  Step: None | Tot: None | Loss: 0.720 | Acc: 75.892% (2808/3700 37/100 ======================================>..........................]  Step: None | Tot: None | Loss: 0.715 | Acc: 75.800% (4548/6000 60/100 =======================================>.........................]  Step: None | Tot: None | Loss: 0.715 | Acc: 75.787% (4623/6100 61/100 ========================================>........................]  Step: None | Tot: None | Loss: 0.711 | Acc: 75.938% (4860/6400 64/100 \n",
      "\n",
      "Epoch: 16\n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.557 | Acc: 80.610% (40305/50000 391/391 \n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.697 | Acc: 76.320% (7632/10000 100/100 ==============>................................................]  Step: None | Tot: None | Loss: 0.731 | Acc: 74.231% (1930/2600 26/100 ================>................................................]  Step: None | Tot: None | Loss: 0.724 | Acc: 74.519% (2012/2700 27/100 =================================>...............................]  Step: None | Tot: None | Loss: 0.713 | Acc: 75.264% (3989/5300 53/100 ==================================>..............................]  Step: None | Tot: None | Loss: 0.712 | Acc: 75.333% (4068/5400 54/100 ======================================>..........................]  Step: None | Tot: None | Loss: 0.715 | Acc: 75.400% (4524/6000 60/100 ==========================================>......................]  Step: None | Tot: None | Loss: 0.708 | Acc: 75.627% (5067/6700 67/100 ===============================================>.................]  Step: None | Tot: None | Loss: 0.706 | Acc: 75.811% (5610/7400 74/100 \n",
      "\n",
      "Epoch: 17\n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.541 | Acc: 81.412% (40706/50000 391/391 \n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.645 | Acc: 77.820% (7782/10000 100/100 ==================>............................................]  Step: None | Tot: None | Loss: 0.651 | Acc: 77.879% (2570/3300 33/100 ==========================>......................................]  Step: None | Tot: None | Loss: 0.649 | Acc: 77.707% (3186/4100 41/100 ===============================>.................................]  Step: None | Tot: None | Loss: 0.646 | Acc: 77.960% (3898/5000 50/100 ==================================>..............................]  Step: None | Tot: None | Loss: 0.647 | Acc: 77.778% (4200/5400 54/100 =======================================>.........................]  Step: None | Tot: None | Loss: 0.645 | Acc: 77.871% (4828/6200 62/100 ===========================================>.....................]  Step: None | Tot: None | Loss: 0.645 | Acc: 77.882% (5296/6800 68/100 ===========================================================>.....]  Step: None | Tot: None | Loss: 0.645 | Acc: 77.892% (7244/9300 93/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 18\n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.532 | Acc: 81.628% (40814/50000 391/391 \n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.603 | Acc: 79.150% (7915/10000 100/100 =====>.........................................................]  Step: None | Tot: None | Loss: 0.626 | Acc: 77.667% (932/1200) 12/100 =======>.........................................................]  Step: None | Tot: None | Loss: 0.616 | Acc: 77.846% (1012/1300 13/100 ===========================>.....................................]  Step: None | Tot: None | Loss: 0.610 | Acc: 78.791% (3388/4300 43/100 ========================================>........................]  Step: None | Tot: None | Loss: 0.604 | Acc: 79.109% (5063/6400 64/100 ===========================================================>.....]  Step: None | Tot: None | Loss: 0.604 | Acc: 79.250% (7291/9200 92/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 19\n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.521 | Acc: 81.830% (40915/50000 391/391 \n",
      " [================================================================>]  Step: None | Tot: None | Loss: 0.853 | Acc: 73.980% (7398/10000 100/100 ====>..........................................................]  Step: None | Tot: None | Loss: 0.858 | Acc: 74.636% (821/1100) 11/100 =======>.........................................................]  Step: None | Tot: None | Loss: 0.857 | Acc: 74.333% (892/1200) 12/100 ========================>........................................]  Step: None | Tot: None | Loss: 0.850 | Acc: 73.895% (2808/3800 38/100 ============================>....................................]  Step: None | Tot: None | Loss: 0.842 | Acc: 74.422% (3349/4500 45/100 ============================================>....................]  Step: None | Tot: None | Loss: 0.859 | Acc: 73.812% (5093/6900 69/100 ===========================================================>.....]  Step: None | Tot: None | Loss: 0.857 | Acc: 73.882% (6871/9300 93/100 ============================================================>....]  Step: None | Tot: None | Loss: 0.857 | Acc: 73.936% (6950/9400 94/100 \n"
     ]
    }
   ],
   "source": [
    "'''Train CIFAR10 with PyTorch.'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import time\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Model\n",
    "print('==> Building model..')\n",
    "# net = VGG('VGG19')\n",
    "net = ResNet50()\n",
    "# net = PreActResNet18()\n",
    "# net = GoogLeNet()\n",
    "# net = DenseNet121()\n",
    "# net = ResNeXt29_2x64d()\n",
    "# net = MobileNet()\n",
    "# net = MobileNetV2()\n",
    "# net = DPN92()\n",
    "# net = ShuffleNetG2()\n",
    "# net = SENet18()\n",
    "# net = ShuffleNetV2(1)\n",
    "# net = EfficientNetB0()\n",
    "# net = RegNetX_200MF()\n",
    "#net = SimpleDLA()\n",
    "net = net.to(device)\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "\n",
    "# Training\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/ckpt.pth')\n",
    "        best_acc = acc\n",
    "start_time = time.time()\n",
    "batch_time = []\n",
    "for epoch in range(start_epoch, start_epoch+20):\n",
    "    start_time_batch = time.time()\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    batch_time.append(time.time() - start_time_batch)\n",
    "    scheduler.step()\n",
    "elapsed_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d038de5f-6109-4793-91e1-005f021c5216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1898.5592563152313"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52abc7b9-a469-4cf0-98df-ba2918158dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[130.0121250152588,\n",
       " 97.95212936401367,\n",
       " 93.42726063728333,\n",
       " 93.26503419876099,\n",
       " 93.37122392654419,\n",
       " 93.52715849876404,\n",
       " 93.47191882133484,\n",
       " 92.5230975151062,\n",
       " 93.472491979599,\n",
       " 92.44763970375061,\n",
       " 92.5136890411377,\n",
       " 92.69960570335388,\n",
       " 92.65980291366577,\n",
       " 92.1723039150238,\n",
       " 92.54834938049316,\n",
       " 92.44543600082397,\n",
       " 92.1417031288147,\n",
       " 92.81850671768188,\n",
       " 92.81309461593628,\n",
       " 92.27039623260498]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38b05729-68a0-46a2-8644-ff94a76cc620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting plotly\n",
      "  Downloading plotly-5.14.1-py2.py3-none-any.whl (15.3 MB)\n",
      "\u001b[K     || 15.3 MB 4.0 MB/s eta 0:00:01    |                               | 174 kB 4.0 MB/s eta 0:00:04     |                           | 2.0 MB 4.0 MB/s eta 0:00:04     |                         | 3.3 MB 4.0 MB/s eta 0:00:04     |                      | 4.5 MB 4.0 MB/s eta 0:00:03     |                    | 5.6 MB 4.0 MB/s eta 0:00:03     |              | 8.6 MB 4.0 MB/s eta 0:00:02     |   | 13.9 MB 4.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tenacity>=6.2.0\n",
      "  Downloading tenacity-8.2.2-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: packaging in /packages/7x/jupyter/2022-04-15/lib/python3.9/site-packages (from plotly) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /packages/7x/jupyter/2022-04-15/lib/python3.9/site-packages (from packaging->plotly) (3.0.8)\n",
      "Installing collected packages: tenacity, plotly\n",
      "Successfully installed plotly-5.14.1 tenacity-8.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c19051a3-6113-45db-8337-33abf5faaa2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dash'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moffline\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpo\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moffline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m download_plotlyjs, init_notebook_mode, plot, iplot\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdash\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpress\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpx\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dash'"
     ]
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.offline as po\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import dash\n",
    "import plotly.express as px\n",
    "import random\n",
    "import plotly.figure_factory as ff\n",
    "from plotly import tools\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly.offline import iplot\n",
    "def line_plot2(X,Y):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=X, y=Y, mode='lines+markers', name='Training Accuracy'))\n",
    "    #fig.add_trace(go.Scatter(x=X, y=SecondY, mode='lines+markers', name='Test Accuracy'))\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title='Parallelism Techniques',\n",
    "        yaxis_title='Time',\n",
    "        legend=dict(\n",
    "            x=0,\n",
    "            y=1,\n",
    "            traceorder=\"normal\",\n",
    "            font=dict(\n",
    "                family=\"sans-serif\",\n",
    "                size=12,\n",
    "                color=\"black\"\n",
    "            ),\n",
    "            bgcolor=\"WhiteSmoke\",\n",
    "            bordercolor=\"Black\",\n",
    "            borderwidth=1\n",
    "        ),\n",
    "        width=800,\n",
    "        height=600,\n",
    "        margin=dict(\n",
    "            l=50,\n",
    "            r=50,\n",
    "            b=50,\n",
    "            t=50,\n",
    "            pad=4\n",
    "        )\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ad1902-e702-47c3-a0de-ef08d903a6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_plot2([0:20],batch_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815dda74-b784-49e3-b785-354d71a31e49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
